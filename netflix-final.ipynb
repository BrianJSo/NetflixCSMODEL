{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix Shows\n",
    "Introduction thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is a collection of information about Netflix shows as of 2019. Each obsevation contains information about a Netflix show's unique ID, its type (i.e. whether it is a TV Show or a movie), title, director, cast, country (where it was produced), release date, release year, TV rating, duration, genre, and description.\n",
    "\n",
    "The dataset 'Netflix Movies and TV Shows' was collected from Flixable, a third-party Netflix search engine. Flixable was created in 2018 by Ville Salminen, and it came with additional advanced search functionality which was missing from the implemented search engine of Netflix. The data was extracted from the Flixable database through the use of API calls.\n",
    "\n",
    "Currently, Netlflix does not have their API publicly available, and Flixable has not openly disclosed how the web site was able to acquire the data for its database. Despite the popularity of the aforementioned site, we cannot confirm whether the dataset that was extracted from Flixable is reliable. Moreover, considering that the latest update for the dataset was on November 2019, the conclusions made in this case study may not be representative of the current Netflix shows as of September, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rule_miner import RuleMiner\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df = pd.read_csv(\"./netflix_titles.csv\")\n",
    "netflix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df['director'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates dito ba?? ewan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for `NAN`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the `description` and `listed_in` variables do not have `Nan`/`null` values. Hence, no need to drop rows (for this question, at least) or do imputation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `description` variable is a good feature to check for duplicates as the synopsis are expectedly unique for each show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_desc_duplicate = netflix_df['description'].value_counts().reset_index(name=\"description\").query(\"description > 1\")\n",
    "print(netflix_desc_duplicate)\n",
    "print(\"Repeated descriptions:\", len(netflix_desc_duplicate))\n",
    "print(\"Repeated shows:\", netflix_desc_duplicate['description'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7 repeated descriptions, and a total of 15 repeated shows based on our assumption that observations with the same description are the same show. To further confirm this, we can look at the other variables of the observations corresponding to these descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df.loc[netflix_df[\"description\"].isin(netflix_desc_duplicate[\"index\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closely examining the observations reveals that the only differences in the observations are the `date_added` and the `title` variables. The date when the show is added doesn't give much insight as it does not give relevant context to the similarity of the show (i.e. different versions of a show can be added at different times, and vice versa). However, the title &mdash; at least in the results above &mdash; are descriptive in the differences.\n",
    "\n",
    "The titles show that there are multiple versions due to langauge. The show *The Incredibles 2*, for instance, has both the original, and the Spanish version. Meanwhile, the movie *Oh! Baby* has two other versions: the Malalayam and Tamil. The only observation which didn't show any difference was for the movie *Sarkar* whose titles (there are 2 observations for this movie) do not indicate versions. This may either be a data collection error, or the title simply does not show which version it is.\n",
    "\n",
    "Nonetheless, all the duplicates will be cleaned in the same way. The \"original\" version (i.e. the one without a parenthesis stating the version, or if not applicable, the first entry that appears in the dataset for simplicity) will be retained. \n",
    "\n",
    "This step is important because duplicates will affect the weight of the words during feature extraction. Thus, there would be bias for a particular show which would produce results that are not representative of the show's listed genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are only few duplicates, we can manually remove the offending observations via `show_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_id_remove_list = [81186758, 81186757, 81072516, 81046962, 81059388, 81151877, 81074135, 81091424]\n",
    "\n",
    "netflix_df.loc[(netflix_df[\"description\"].isin(netflix_desc_duplicate[\"index\"])) & (~netflix_df[\"show_id\"].isin(show_id_remove_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the observations that will be retained from all the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df_clean = netflix_df[~netflix_df[\"show_id\"].isin(show_id_remove_list)]\n",
    "netflix_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) What genres appear most frequently among Netflix shows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first exploratory question, we decided to determine what genres are the most frequent among Netflix shows.\n",
    "\n",
    "The genres of each Netflix show is shown in the `listed_in` column of the dataset in a single string format. Each string can contain more than one genre, and they are spearated by commas. To be able to count the frequency of each genre in the dataset, the `listed_in` column must be parsed into another table with each row containing all genres of a show, and each column containing only a single genre.\n",
    "\n",
    "First we take the `listed_in` column of the dataset and isolate the genres per Netflix show by splitting the strings at commas with spaces after them (, )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_in_series = netflix_df_clean['listed_in']\n",
    "genre_matrix = []\n",
    "\n",
    "for string in listed_in_series:\n",
    "    split_str = string.split(', ')\n",
    "    genre_matrix.append(split_str)\n",
    "\n",
    "genre_df = pd.DataFrame(genre_matrix)\n",
    "genre_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a table with 6226 observations and 3 columns, which means that the maximum number of genres that netflix shows are listed in is only 3. Knowing this, we can create a series by concatenating each column into one single column. Then we can drop the values which are 'None', thus cleaning the resulting series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_list = pd.concat([genre_df[0], genre_df[1], genre_df[2]])\n",
    "genre_list.dropna(inplace = True)\n",
    "genre_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the genres have been isolated, we can now use a bar plot to visualize the number of times a particular genre appeared on a Netflix show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genre_count = genre_list.value_counts()\n",
    "\n",
    "genre_count.plot.bar()\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bar plot of genre count in Netflix shows')\n",
    "genre_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bar plot, we can see the ranking of the shows based on its frequency in Netflix shows. We see that 'International Movies' appear most frequently with almost 1922 appearances in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) What is the trend on the number of International Movies per year "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the bar plot of the genre frequency, 'International Movies' was determined to be the most frequent genre in the dataset. Knowing this, the second exploratory question will focus on the trend of 'International Movies' according to its release year in the Netflix dataset. First we need to create a dataframe which only contain the shows which are listed as 'International Movies' in the dataset. We also no longer need to perform additional data cleaning, since the dataset has no null values for `listed_in` or `release_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netflix_df_international = netflix_df_clean\n",
    "netflix_df_international.insert(12, \"isInternationalMV\", netflix_df_clean[\"listed_in\"].str.find(\"International Movies\") != -1, True)\n",
    "netflix_df_international = netflix_df_international[netflix_df_international.isInternationalMV]\n",
    "netflix_df_international"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe which only contains the shows which have been listed as 'International Movies', we can now use a histogram in order to visualize the trend of shows listed as 'International Movies' in the Netlflix dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df_international['release_year'].hist(bins=60)\n",
    "plt.xlabel('Release Year')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of International Movies per year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the histogram above, we can see that within the Netflix catalog, there is a large number of shows listed as 'International Movies' which were released towards the latter half of the 2010's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.) What countries produce the most 'International Movies'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that our previous exploratory question answered that shows listed as 'International Movies' were the most prominent in the Netflix dataset, the third exploratory question would determine which countries these shows belong to. \n",
    "\n",
    "Since we already have a dataframe which contains all the shows listed as 'International Movies', we can use that dataframe for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df_international['country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `country` column reveals that there are some instances in which a show was produced in more than one country and these countries are separated by commas. As such, we need to parse the `country` column into another table with each row containing all countries of a show, and each column containing only a single country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df_international_country = netflix_df_international.dropna(subset=['country'])\n",
    "international_series = netflix_df_international_country['country']\n",
    "international_matrix = []\n",
    "\n",
    "for string in international_series:\n",
    "    split_str = string.split(', ')\n",
    "    international_matrix.append(split_str)\n",
    "\n",
    "international_df = pd.DataFrame(international_matrix)\n",
    "international_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a table with 1854 observations and 11 columns, which means that the maximum number of countries that the shows in our dataframe were produced in is 11. As such, we can create a series by concatenating each column into one single column. Then we can drop the values which are 'None', thus cleaning the resulting series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = pd.concat([international_df[0], international_df[1], international_df[2], international_df[3], international_df[4], international_df[5], international_df[6], international_df[7], international_df[8], international_df[9], international_df[10], international_df[11]])\n",
    "country_list.dropna(inplace = True)\n",
    "country_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the countries have been separated, we can now use a bar plot to visualize the distribution of shows listed as 'International Movies' based on the country where it was produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_count = country_list.value_counts()\n",
    "\n",
    "country_count.plot.bar()\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bar plot of distribution of International Movies based on country')\n",
    "country_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most common association rules among Netflix genres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `listed_in` column in the dataset refers to what genres the shows are listed in on the streaming platform. The values in this column are represented as strings where each string can contain more than one genre, and each genre is separated by commas. To parse these genres we need to create a matrix that is a list of genre lists, where each row represents the genres of one show. We can then use this matrix to create a dataframe that we can manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_in_series = netflix_df_clean['listed_in']\n",
    "genre_matrix = []\n",
    "\n",
    "for string in listed_in_series:\n",
    "    split_str = string.split(', ')\n",
    "    genre_matrix.append(split_str)\n",
    "\n",
    "genre_df = pd.DataFrame(genre_matrix)\n",
    "genre_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a value dictionary to assign a number to each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = genre_df.values.ravel()\n",
    "values = [value for value in pd.unique(values) if not pd.isnull(value)]\n",
    "\n",
    "value_dict = {}\n",
    "for i, value in enumerate(values):\n",
    "    value_dict[value] = i\n",
    "value_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the dictionary, we can then create a dataframe where each row is a netflix show and each column is a genre. If the show is listed in that genre then the value for that column is `1` and `0` if it is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = genre_df.stack().map(value_dict).unstack()\n",
    "\n",
    "baskets = []\n",
    "for i in range(genre_df.shape[0]):\n",
    "    basket = np.sort([int(x) for x in genre_df.iloc[i].values.tolist() if str(x) != 'nan'])\n",
    "    baskets.append(basket)\n",
    "\n",
    "shows_df = pd.DataFrame([[0 for _ in range(len(value_dict))] for _ in range(len(baskets))], columns=values)\n",
    "\n",
    "for i, basket in enumerate(baskets):\n",
    "    shows_df.iloc[i, basket] = 1\n",
    "shows_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing the dataframe, we can finally use a **Rule Miner** to figure out the common genre association rules of netflix shows. There can be many configurations of this Rule Miner that will produce different results. Because of this, multiple tests will be done on the dataset which will fulfill different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first configuration will be setting the support threshold to the **lowest occurence** of a genre. This will be done to accomodate all genres in the dataset since all genres have appeared that many times. To do this we must first get all the total counts of each genre, then get the lowest value from the results. Since the total count of each genre has already been generated in the Exploratory Data Analysis, all we need to do is get the minimum value from that series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last 10 results of the series\\n\")\n",
    "print(genre_count.tail(10),\"\\n\")\n",
    "print(\"Minimum no. of occurrences:\", genre_count.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum value of the total genre counts is `10`, which will be set as the support threshold.\n",
    "\n",
    "The confidence level is then lowered starting from `100%` until it produces at least 1 association rule\n",
    "\n",
    "From this result, we initialize the first Rule Miner to have a **support threshold** of `10` and a **confidence** of `100%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_miner_lowest_occur = RuleMiner(10, 1)\n",
    "assoc_rules_lowest_occur = rule_miner_lowest_occur.get_association_rules(shows_df)\n",
    "assoc_rules_lowest_occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result of the get_association_rules function of the rule miner, we can see that the genre association rules are:\n",
    " - ['Crime TV Shows', 'Korean TV Shows'] → ['International TV Shows']\n",
    " - ['International TV Shows', 'Science & Nature TV'] → ['Docuseries']\n",
    " - ['Science & Nature TV', 'International TV Shows'] → ['Docuseries']\n",
    " - ['Romantic TV Shows', 'Korean TV Shows'] → ['International TV Shows']\n",
    " - ['TV Dramas', 'Korean TV Shows'] → ['International TV Shows']\n",
    " - ['British TV Shows', 'Science & Nature TV'] → ['Docuseries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second configuration will be setting the support threshold to the **average occurence** of all genres. This will be done to make sure that the genres that will be included in the rules must have occured a sufficient number of times. We can reuse the total counts of each genre and get the mean value from the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average no. of occurrences:\", genre_count.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum value of the total genre counts is `324.93`, which we can round off and set as the support threshold.\n",
    "\n",
    "The confidence level is then lowered starting from `100%` until it produces at least 1 association rule\n",
    "\n",
    "From this result, we initialize this Rule Miner to have a **support threshold** of `325` and a **confidence** of `74%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_miner_mean_occur = RuleMiner(325, 0.74)\n",
    "assoc_rules_mean_occur = rule_miner_mean_occur.get_association_rules(shows_df)\n",
    "assoc_rules_mean_occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result of the get_association_rules function of the rule miner, we can see that the only genre association rule produced is:\n",
    " - ['Independent Movies'] → ['Dramas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code only\n",
    "\n",
    "rule_miner_mean_occur = RuleMiner(325, 0.60)\n",
    "assoc_rules_mean_occur = rule_miner_mean_occur.get_association_rules(shows_df)\n",
    "assoc_rules_mean_occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Netflix synopses informative enough to classify whether shows belong to the International Movies genre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `description` variable in the dataset refers to Netflix's synopsis of each show. For this research question, the idea is to determine whether these synopses are informative enough to classify whether a show is classified under *International Movies*. This entails building a classifier which would be able to identify all *International Movie* shows in the dataset.\n",
    "\n",
    "This is interesting because the problem leads to more questions about how informative Netflix synopses are. Are plots &mdash; represented by the synopses &mdash; dependent of genre? Are stories similar for the same genre? Are words able to represent genres? These will not be answered in this research problem, but are indications of the many stimulating possibilities that this research problem entails. It's also an indication of the capabilities of computers in handling natural language.\n",
    "\n",
    "The *International Movies* genre is chosen because 1,922 shows are classified under this genre which makes it the most prominent. Choosing a prominent genre is important because there is a need to have a good amount of samples; this is especially true for classification which requires good representatives for each categories &mdash; in this case, the categories are \"International Movies\" and \"Not International Movies\". For this problem, only the `description` and `listed_in` variables from the Netflix dataset will be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this classification, two sets of data are needed: data which contains only shows with *International Movies* as its genre (regardless whether it's also classified as other genres), and data containing shows which are not classified under *International Movies*.\n",
    "\n",
    "In the exploratory data analysis, there's already a `DataFrame` pertaining to the first. What we need is a `DataFrame` for shows that aren't classified under *International Movies*. To do this, the `isInternationalMV` variable from `netflix_df_clean`, can be used where False values indicates that show is not under said classification. After getting the False values, the next step is to concatenate both \"Not International Movies\" and \"International Movies\" data together to prepare it for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netflix_df_non_international = netflix_df_clean[netflix_df_clean[\"isInternationalMV\"] == False]\n",
    "netflix_df_non_international"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netflix_df_classifier = pd.concat([netflix_df_non_international, netflix_df_international])\n",
    "netflix_df_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"International Movies:\", len(netflix_df_international))\n",
    "print(\"Not International Movies:\", len(netflix_df_non_international))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Total International Movies shows:** 1922 shows\n",
    "\n",
    "- **Total Not International Movies shows:** 4304 shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "The `description` variable is in the form of sentences (naturally, because synopses are written with words). The task is to evaluate the value of words in these synposes.  However, computers do not really understand sentences. Hence, a mathematical model is important to transform the words into a numerical representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "The term frequency-inverse document frequency (TF-IDF) is a statistical method which places value in words through an inverse proportion of the word's frequency in a document, in this case the `description`, to the amount of documents the word appears in. The importance of a word is indicated by a high TF-IDF value. The TF-IDF value is given by the formula: \n",
    "\n",
    "$$W_{i,j} = tf_{i,j} \\times log{N \\over df_{i}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$tf_{i,j}$ = number of occurences of $i$ in $j$,\n",
    "\n",
    "$df_{i}$ = number of documents containing $i$, and\n",
    "\n",
    "$N$ = total number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn's [TF-IDF Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) was utilized to transform the `description` column into a matrix of TF-IDF features. First we fit the documents (i.e. the synposes) by using the vectorizer's `fit` function for the vectorizer to learn the vocabulary. Once vectorizer knows the vocabulary and document frequencies, the `transform` function is used to transform the documents into a document-term matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(netflix_df_classifier[\"description\"])\n",
    "features=vectorizer.transform(netflix_df_classifier[\"description\"])\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the TF-IDF Vectorizer converted the `description` column into a sparse matrix of TF-IDF values where each row contains 16411 columns. This also means that there are 16411 words that were extracted as features from the dataset. To confirm this, we can get the word form of the features from the vectorizer by using its `get_feature_names` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_words = vectorizer.get_feature_names()\n",
    "print(features_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a list of all the words, sorted alphabetically, that the vectorizer learned and, consequently, used as features to build the sparse matrix. Results show that there are indeed 16411 features for each row (and thus, for each show)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing labels\n",
    "In order for the classifier to classify, it should be trained knowing the labels &mdash; in this case *International Movie* or *Not International Movie* &mdash; of the show it's trying to classify. The next step is to prepare these labels by utilizing the `isInternationalMV` variable of the dataset to form an array of labels. *International Movie* shows are represented as 1s, while *Not International Movie* shows are represented as 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for show in netflix_df_classifier[\"isInternationalMV\"]:\n",
    "    if show == True:\n",
    "        appenddata=1\n",
    "    if show == False:\n",
    "        appenddata=0\n",
    "    labels.append(appenddata)\n",
    "    \n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because *Not International Movie* shows and *International Movie* shows were simply concatenated, it's expected that the dataset is split homogenously where the first parts are all *Not International Movie* shows and the latter are *International Movie* shows. This is not a problem, however, since a stratified cross-validation approach will be utilized which splits the data such that there will be close-as-possible distribution of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classifier and measuring performance\n",
    "This [chart](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) from scikit-learn was utilized to choose the right estimator for this problem. The circumstances of this research lead us to selecting [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) from scikit-learn's support vector machine module.\n",
    "\n",
    "Feeding the data to the machine algorithm can be manually done by using the functions of the `LinearSVC` class. However, as mentioned earlier, cross-validation will be used in evaluating the performance of the dataset. For this purpose,  [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) will allow us to both build the classifier, and evaluate its performance at the same time. For binary classification problems such as what we have, `cross_validate` automatically uses the [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) as its cross-validation splitter, hence, we minimize having bias in each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize the estimator (Linear SVC), and name the metrics/scoring that will be used by the cross-validation in evaluating performance. Default parameters were used for the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearSVC=svm.LinearSVC()\n",
    "scoring = ['accuracy', 'recall', 'precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we simply use the estimator, the features, the labels, and the scorers as parameters for `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(linearSVC, features, labels, scoring=scoring)\n",
    "print('Mean Accuracy:', scores['test_accuracy'].mean())\n",
    "print('Accuracy Two Standard Deviations:', scores['test_accuracy'].std() * 2)\n",
    "print('Mean Precision', scores['test_precision'].mean())\n",
    "print('Mean Recall', scores['test_recall'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **95% Confidence Interval of Genre Classifier Accuracy:** 72.23% $\\pm$ 2.98\n",
    "\n",
    "- **Average Genre Classifier Precision:** 57.83%\n",
    "\n",
    "- **Average Genre Classifier Recall:** 39.80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more information, we can extract the top features by using [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html?highlight=selectkbest#sklearn.feature_selection.SelectKBest) which returns the top *k* features based on a scoring function. In this case, we will use [`Chi-squared statistics`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2)because it is expected that the features &mdash; in the form of TF-IDF values &mdash; will never be negative. Furthermore, because chi-squared statistics measures dependence between variables, it can be used to determine which features are independent of classes, hence, not useful in classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=20)\n",
    "selector.fit(features, labels)\n",
    "# Get idxs of columns to keep\n",
    "top_features_idxs = selector.get_support(indices=True)\n",
    "\n",
    "scores, pval=(chi2(features, labels))\n",
    "\n",
    "chi2_data={'Feature Names': np.array(features_words)[top_features_idxs], 'Chi2': scores[top_features_idxs], 'P-value': pval[top_features_idxs]}\n",
    "features_chi2 = pd.DataFrame(chi2_data).sort_values(by=['Chi2'], ascending=False, ignore_index=True)\n",
    "features_chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataFrame` shows the top 20 most class-dependent features identified through chi-squared statistics. When the classifier sees these words in the synopses, there is a high probability that it would be able to classify whether the show is an *International Movie* or not correctly.\n",
    "\n",
    "Note, however, that the results are based on the evaluation of the data as a whole (i.e. did not follow the cross-validation scheme), and without consideration of the machine learning algorithm. The implication is that it may not be representative of the the accuracy of the classifiers that were measured during cross-validation. Nevertheless, this method still shows a general idea of what the top features would be for a classifier that is built with synopses and genre data from the Netflix dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "The 95% confidence interval for a Netflix genre classifier's accuracy is 72.23% $\\pm$ 2.98. This means that the classifier, more than half of the time, was able to classify whether a show is an *International Movie* or *Not International Movie*. Only considering accuracy, this can be interpreted as successful.\n",
    "\n",
    "However, even though a stratified cross-validation was used, there is still some class imbalance because the dataset contains 1,922 *International Movie* shows, and 4,304 *Not International Movie* shows. The latter class is more than twice the other. This renders the accuracy measure an inferior evaluator because it can yield a high accuracy even if it fails to classify a lot of International Movie shows.\n",
    "\n",
    "For instance, if the classifier were to correctly guess all *Not International Movie* shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intl_total = 1922\n",
    "non_intl_total = 4304\n",
    "    \n",
    "non_intl_total / (intl_total + non_intl_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy would be 69.13% which can still be interpreted as the classifier being able to correctly classify more than half the time. Given so, we can look at [precision and recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall). The former is concerned with the correct *International Movie* show predictions, while the latter is concerned with the number of actual *International Movie* shows that were identified by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall\n",
    "The average precision for the classifiers is 57.83%. This means that more than half the time, when the classifiers predict a show to be under *International Movies*, it is true. Meanwhile, the average recall for the classifiers is 39.80%. This means that the classifiers were only able to identify less than half of all the *International Movie* shows. Given a standard of 50%, the classifiers were not successful at all.\n",
    "\n",
    "Given that the goal was to correctly identify all *International Movies* shows from the data, precision is much less important than recall. As such, looking at recall, we can say that although accuracy is high, the classifiers are not able to satisfactorily identify the *International Movie* shows.\n",
    "\n",
    "However, this means that accuracy is truly affected by the class imbalance. The implication is that the classifier were probably able to classify *Not International Movie* shows, at least better than its counterpart. To test this we can flip the labels to say that the true positives are the *Not International Movie* shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_labels=[]\n",
    "for show in netflix_df_classifier[\"isInternationalMV\"]:\n",
    "    if show == True:\n",
    "        appenddata=0\n",
    "    if show == False:\n",
    "        appenddata=1\n",
    "    invert_labels.append(appenddata)\n",
    "    \n",
    "invert_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_scores = cross_validate(linearSVC, features, invert_labels, scoring=scoring)\n",
    "print('Mean Precision', invert_scores['test_precision'].mean())\n",
    "print('Mean Recall', invert_scores['test_recall'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Average Precision:** 76.34%\n",
    "\n",
    "- **Average Recall:** 86.71%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show an astounding increase in performance. Recall increased more than twice which indicates that the classifiers were able to predict most of the *Not International Movie* shows in the dataset. \n",
    "\n",
    "Thus, it can still be said that synopses are still informative in genre classification, just the other way around. In future studies, a better way to test this is to have a dataset that have equal, or close distribution of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Features\n",
    "The top class-dependent features also gives insight about the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table, there words that can be identified to match with the idea of an international movie:\n",
    "- Bollywood\n",
    "- India\n",
    "- Indian\n",
    "- Mumbai\n",
    "- Caste\n",
    "\n",
    "Under a 0.05 significance level, these are all hugely relevant to the classification process. It's also worth noting that these words are all related to India. This result is consistent with the findings in the exploratory data analysis stating that most of the shows that fall under the *International Movies* genre &mdash; 714 shows to be exact &mdash; is produced in India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it is expected that the chi-squared statistics, given a set of *International Movies* shows, most of which are produced in India, would find Indian-related words to be relevant features for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, based on analysis, it can be said that Netflix synopses are not informative enough in classifying if shows belong to the International Movies genre. However, if the goal was identifying the opposite, Netflix synopses prove to be informative. In this latter context, performance metrics reveal that the classifier was able to perform well given only show synopses and their corresponding genre. \n",
    "\n",
    "Meanwhile, a brief top-features analysis show that synopses contain information, specifically words, that are relevant to its genre, and consequently, the classification. \n",
    "\n",
    "In the future, it is recommended to conduct parameter tuning for the estimator to see whether there are better parameters that would yield better classifier performance; a better performance may conclude that the problem was not due to the dataset, but with the classifier's setup. Another recommendation is to consider other metrics such as the confusion matrix to further analyze results and why the classifier behaves the way it does. It is also recommended that further data preprocessing techniques be done such as lemmatization, and stop words removal. Finally, in line with the chi-squared statistics of finding the top features, further analysis can be done to find out which words can describe *International Movies* in Netflix. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
